

| **Setting**               | **Description**                                                                                                               | **Default** | **Example**                       | **Invalid Operations**                                          | **Input For**                               |
|---------------------------|-------------------------------------------------------------------------------------------------------------------------------|-------------|-----------------------------------|----------------------------------------------------------------|--------------------------------------------|
| **URL File**              | The name of the text file containing the URLs to crawl. Each URL should be on a new line. This file is essential for the crawler to know where to start. | `urls.txt`  | `youtube_links.txt`               | File does not exist, file is empty, or contains invalid URLs   | Name of the file to be read (e.g., `urls.txt`) |
| **Number of Connections** | The maximum number of simultaneous connections the crawler can make when fetching pages. Increasing this number can speed up the crawl but may also lead to throttling or blocking by websites. | `10`        | `5` (for fewer simultaneous requests) | Negative numbers, non-integer values                           | Integer value representing max connections  |
| **Timeout**               | The maximum time in seconds that the crawler will wait for a server response before considering the request failed. This setting helps manage slow servers or unresponsive pages. | `40`        | `30` (if the server is slow to respond) | Negative numbers, non-integer values                           | Integer value representing timeout in seconds |
| **Search Links**          | A boolean flag that indicates whether the crawler should extract links from the crawled pages. If set to `On`, the crawler will look for and collect all links found on each page. | `On`        | `Off` (to disable link extraction) | Values other than `On` or `Off`                                | `On` or `Off` to toggle link extraction    |
| **Output Directory**      | The directory where the crawler will save its output files, including logs, heatmaps, and graphs. This helps in organizing the results of the crawl. | `output`    | `results` (to save in a different folder) | Invalid directory paths, paths to files instead of directories   | Path where output files will be saved      |
| **Crawl Depth**           | The maximum number of levels the crawler will traverse from the original URLs. A depth of `1` means it will only crawl the provided URLs, while `2` will crawl links found on those pages. | `3`         | `2` (to limit the crawl to 2 levels) | Negative numbers, non-integer values                           | Integer value representing max crawl depth  |
| **User Agents**           | A list of user agent strings that the crawler can use to simulate different browsers or devices when making requests. This can help avoid being blocked by websites that filter traffic based on user agent. | `3 agents`  | `["Mozilla/5.0 ... Chrome/58.0.3029.110", "Mozilla/5.0 ... Firefox/54.0"]` | Empty list, invalid user agent formats                           | Comma-separated list of user agents        |
| **Resume Crawl**          | A boolean flag that indicates whether the crawler should continue from where it left off in case of a previous interrupted run. If set to `On`, it will try to resume crawling from previously visited URLs. | `Off`       | `On` (to allow continuation of interrupted crawls) | Values other than `On` or `Off`                                | `On` or `Off` to toggle resume feature      |

